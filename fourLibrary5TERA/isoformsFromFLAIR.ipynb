{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# isoformsFromFLAIR.ipynb\n",
    "## Marcus Viscardi,    January 21, 2023\n",
    "\n",
    "So on Friday Jan 20th, I reran the pipelines for all four new libs and the two pilot libraries with the tag for FLAIR analysis turned on. Here I want to try and dig into those FLAIR outputs and see if I can pick out any isoform dependent effects such as NMD-sensitivity, or tail length differences!\n",
    "\n",
    "Large detail here. FLAIR adds a transcript_id tag, but this doesn't necessarily match the gene_id tag carried over from featureCounts (I *think*). This means I'll need to add a step to \"back calculate\" the gene_id/_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sea\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.insert(0, '/data16/marcus/scripts/nanoporePipelineScripts')\n",
    "import nanoporePipelineCommon as npCommon\n",
    "from nanoporeReadPlotting.finalizingReadAndCoveragePlotting_matplotlib import plot_reads, coverage_plotting_5tera\n",
    "\n",
    "CONVERSION_DICT = {\"xrn-1-5tera\": \"oldN2\",\n",
    "                   \"xrn-1-5tera-smg-6\": \"oldS6\",\n",
    "                   \"5tera_xrn-1-KD_wt\": \"newN2\",\n",
    "                   \"5tera_xrn-1-KD_smg-5\": \"newS5\",\n",
    "                   \"5tera_xrn-1-KD_smg-6\": \"newS6\",\n",
    "                   \"5tera_xrn-1-KD_smg-7\": \"newS7\",\n",
    "                   \"5tera_xrn-1-KD_wt_rerun\": \"newerN2\",\n",
    "                   \"sPM57\": \"sPM57\",\n",
    "                   \"sPM58\": \"sPM58\",\n",
    "                   }\n",
    "REV_CONVERSION_DICT = {val: key for key, val in CONVERSION_DICT.items()}\n",
    "\n",
    "from typing import List\n",
    "\n",
    "def load_flair_and_filter_assignments_with_genes(target_lib_name: str,\n",
    "                                                 trust_flair_assignment=True) -> pd.DataFrame:\n",
    "    # Load the transcripts tsv and the genes parquet:\n",
    "    lib_txns_df = npCommon.adjust_5_ends(pd.read_table(npCommon.pick_lib_return_path(target_lib_name, file_midfix='_mergedWithTranscripts', file_suffix='.tsv'))).set_index('read_id')\n",
    "    lib_genes_df = npCommon.adjust_5_ends(pd.read_parquet(npCommon.pick_lib_return_path(target_lib_name, file_midfix='_mergedOnReads', file_suffix='.parquet'))).set_index('read_id')\n",
    "    # Also load my pre-parsed gtf file which will allow us to convert the FLAIR transcript_id to gene_ids/_names/_biotypes!\n",
    "    gtf_df = pd.read_parquet(\"/data16/marcus/genomes/elegansRelease100/Caenorhabditis_elegans.WBcel235.100.gtf.parquet\")[['chr', 'feature', 'gene_id', 'gene_name', 'gene_biotype', 'transcript_id', 'transcript_biotype']]\n",
    "    gtf_df = gtf_df.query(\"feature == 'transcript'\")[['transcript_id', 'gene_id', 'gene_name', 'gene_biotype', 'transcript_biotype']]\n",
    "    # Merge those:\n",
    "    lib_txns_df = lib_txns_df.reset_index().merge(gtf_df, on='transcript_id', how='left',\n",
    "                                                  suffixes=('_original', ''))\n",
    "    \n",
    "    # Merge the FLAIR dataframe and the gene assignment info from the gene dataframe:\n",
    "    lib_txns_extended_df = pd.merge(lib_txns_df,\n",
    "                                    lib_genes_df[['gene_id', 'gene_name', 'chr', 'chr_pos']].reset_index(),\n",
    "                                    on=['read_id', 'chr', 'chr_pos'],\n",
    "                                    # inner vs. outer here really just changes whether on not we carry over all the reads that made it through to the gene table but not the transcript table!\n",
    "                                    how='left',\n",
    "                                    suffixes=('', '_fromGeneAssign')).set_index('read_id')\n",
    "    \n",
    "    # Perform the filtering for matched assignment between both methods. This is conservative, but at least it avoids issues arising from FLAIR not giving us enough info to figure out which read on multimappers we are looking at!\n",
    "    if trust_flair_assignment:\n",
    "        print(f\"We are trusting the flair assignment despite {lib_txns_extended_df[lib_txns_extended_df.gene_id != lib_txns_extended_df.gene_id_fromGeneAssign].shape[0]} genes not matching with gene assignment!\")\n",
    "    else:\n",
    "        read_count_before_filter = lib_txns_extended_df.shape[0]\n",
    "        lib_txns_extended_df = lib_txns_extended_df[lib_txns_extended_df.gene_id == lib_txns_extended_df.gene_id_fromGeneAssign]\n",
    "        read_count_after_filter = lib_txns_extended_df.shape[0]\n",
    "        reads_lost = read_count_before_filter - read_count_after_filter\n",
    "        print(f\"After filtering for reads where both FLAIR and geneAssign/featureCount assignments match, {read_count_after_filter:,} of {read_count_before_filter:,} reads remain.\\n\"\n",
    "              f\"This is a loss of {reads_lost:,} reads, or {reads_lost / read_count_before_filter:0.2%}\")\n",
    "    lib_txns_extended_df['lib'] = CONVERSION_DICT[target_lib_name]\n",
    "    return  lib_txns_extended_df.reset_index()\n",
    "\n",
    "\n",
    "def long_to_wide(input_df, wide_target_cols=['gene_rpm'], expand_col='lib') -> pd.DataFrame:\n",
    "    w = input_df[wide_target_cols].unstack(level=expand_col)\n",
    "    w.columns = w.columns.map('{0[0]}_{0[1]}'.format)\n",
    "    return w.reset_index().fillna(0)\n",
    "\n",
    "def compress_df(input_df, keep_transcript_info=True, group_by_t5=True, additional_groupby_columns: List[str] = None, calc_protein_coding_rpm=True):\n",
    "    groupby_col_list = [\"lib\",\n",
    "                        \"chr_id\",\n",
    "                        \"gene_id\",\n",
    "                        \"gene_name\"]\n",
    "    print(f\"Creating groupby dataframe merged on: {groupby_col_list}\")\n",
    "    if keep_transcript_info:\n",
    "        print(f\"\\t+ [transcript_id]\")\n",
    "        groupby_col_list.append(\"transcript_id\")\n",
    "    if group_by_t5:\n",
    "        print(f\"\\t+ [t5] tag\")\n",
    "        groupby_col_list.append(\"t5\")\n",
    "    if additional_groupby_columns:\n",
    "        for additional_col in additional_groupby_columns:\n",
    "            print(f\"\\t+ [{additional_col}]\")\n",
    "            groupby_col_list.append(additional_col)\n",
    "    # Holy crap, the observed=True helps to keep this from propagating out to 129,151,669,691,968 rows...\n",
    "    groupby_obj = input_df.groupby(groupby_col_list, observed=True)\n",
    "    # Change the compressed prefix so that I am count gene hits or transcript hits, depending on set up!\n",
    "    if not keep_transcript_info:\n",
    "        compressed_prefix = \"gene\"\n",
    "    else:\n",
    "        compressed_prefix = \"transcript\"\n",
    "    tqdm.pandas(desc=f\"Counting reads per {compressed_prefix}\")\n",
    "    compressed_df = groupby_obj[\"read_id\"].progress_apply(len).to_frame(name=f\"{compressed_prefix}_hits\")\n",
    "    \n",
    "    compressed_df[\"mean_polya_length\"] = groupby_obj[\"polya_length\"].mean()\n",
    "    compressed_df[\"median_polya_length\"] = groupby_obj[\"polya_length\"].median()\n",
    "    compressed_df[\"called_polya_count\"] = groupby_obj[\"polya_length\"].count()\n",
    "    compressed_df[\"called_polya_frac\"] = compressed_df[\"called_polya_count\"] / compressed_df[f\"{compressed_prefix}_hits\"]\n",
    "    \n",
    "    compressed_df[\"mean_read_length\"] = groupby_obj[\"read_length\"].mean()\n",
    "    compressed_df[\"median_read_length\"] = groupby_obj[\"read_length\"].median()\n",
    "    # RPM and fractional hits calculations\n",
    "    # Need to first create columns of NA values, tobe overwritten\n",
    "    compressed_df[f\"{compressed_prefix}_rpm\"] = pd.NA\n",
    "    compressed_df[f\"{compressed_prefix}_frac_hits\"] = pd.NA\n",
    "    if group_by_t5:\n",
    "        compressed_df[f\"{compressed_prefix}_t5group_rpm\"] = pd.NA\n",
    "    if calc_protein_coding_rpm:\n",
    "        compressed_df[f\"{compressed_prefix}_proteinCoding_rpm\"] = pd.NA\n",
    "        gene_id_and_biotype_df = pd.read_parquet(\"/data16/marcus/genomes/elegansRelease100/Caenorhabditis_elegans.WBcel235.100.gtf.parquet\")[['gene_id', 'gene_biotype']].drop_duplicates()\n",
    "        compressed_df = compressed_df.reset_index().merge(gene_id_and_biotype_df, on='gene_id', how='left').set_index(compressed_df.index.names)\n",
    "    # Only look at one library at a time (so the normalization is per lib not whole df)\n",
    "    for lib in compressed_df.index.unique(level='lib').to_list():\n",
    "        # Create the 'norm_factor' which will be the total # of read hits in that lib\n",
    "        norm_factor = compressed_df.query(f\"lib == '{lib}'\")[f\"{compressed_prefix}_hits\"].sum()\n",
    "        # Turn the total number of read hits into the 'million of read hits'\n",
    "        rpm_norm_factor = norm_factor / 1000000\n",
    "        # For each library divide gene_hits by the rpm norm factor to get rpm\n",
    "        rpm_series = compressed_df.query(f\"lib == '{lib}'\")[f\"{compressed_prefix}_hits\"] / rpm_norm_factor\n",
    "        # Use a series fill, so that we can fill that library's part of the DF without effecting others\n",
    "        compressed_df[f\"{compressed_prefix}_rpm\"] = compressed_df[f\"{compressed_prefix}_rpm\"]. \\\n",
    "            fillna(value=rpm_series)\n",
    "        # Same as above, but with fraction of hits, rather than a rpm calc (practically same thing)\n",
    "        gene_frac_hits_series = compressed_df.query(f\"lib == '{lib}'\")[f\"{compressed_prefix}_hits\"] / norm_factor\n",
    "        compressed_df[f\"{compressed_prefix}_frac_hits\"] = compressed_df[f\"{compressed_prefix}_frac_hits\"]. \\\n",
    "            fillna(value=gene_frac_hits_series)\n",
    "        if group_by_t5:\n",
    "            # We can also calculate an adapted-specific RPM:\n",
    "            for adapted_or_not in [\"+\", \"-\"]:\n",
    "                norm_factor = compressed_df.query(f\"lib == '{lib}'\")\\\n",
    "                    .query(f\"t5 == '{adapted_or_not}'\")[f\"{compressed_prefix}_hits\"].sum()\n",
    "                rpm_norm_factor = norm_factor / 1_000_000\n",
    "                rpm_series = compressed_df.query(f\"lib == '{lib}'\")\\\n",
    "                    .query(f\"t5 == '{adapted_or_not}'\")[f\"{compressed_prefix}_hits\"] / rpm_norm_factor\n",
    "                compressed_df[f\"{compressed_prefix}_t5group_rpm\"] = compressed_df[\n",
    "                    f\"{compressed_prefix}_t5group_rpm\"].fillna(value=rpm_series, axis='index')\n",
    "        if calc_protein_coding_rpm:\n",
    "            protein_coding_norm_factor = compressed_df.query(f\"lib == '{lib}'\").query(f\"gene_biotype == 'protein_coding'\")[f\"{compressed_prefix}_hits\"].sum()\n",
    "            protein_coding_rpm_norm_factor = protein_coding_norm_factor / 1000000\n",
    "            protein_coding_rpm_series = compressed_df.query(f\"lib == '{lib}'\")[f\"{compressed_prefix}_hits\"] / protein_coding_rpm_norm_factor\n",
    "            compressed_df[f\"{compressed_prefix}_proteinCoding_rpm\"] = compressed_df[f\"{compressed_prefix}_proteinCoding_rpm\"]. \\\n",
    "                fillna(value=protein_coding_rpm_series)\n",
    "    return compressed_df\n",
    "\n",
    "print(f\"Imports done at {npCommon.get_dt(for_print=True)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First lets check to see if my usual method and the FLAIR method at least assigned things to the same genes!\n",
    "\n",
    "The detail here is that I need to reverse look up the gene_id from the transcript_id that FLAIR provided!!\n",
    "\n",
    "So, messing around with the FLAIR dataframe has helped me realize/remember an issue. The FLAIR outputs only spit out read_id and transcript_id (at least in the way that I hacked it together). This means that any multimapping reads will have no means to resolve which assignment went with which map! This isn't a large scale issue at the moment, in this case (newN2) I can see 76.5k reads that match gene_ids from gene analysis and FLAIR, but ~18.3k are assigned by FLAIR but not my method, another ~500 reads are explicitly mismatched between the methods. I can toss everything besides the 76.5k out of 95.3k reads\n",
    "\n",
    "For now, we will go with the conservative method, throw out everything that doesn't perfectly match. Maybe I'll have this spit out a percentage lost when doing this for my records."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load libs:\n",
    "\n",
    "If there isn't an avaible preprocessed file, this lib loading step will take a little."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "regenerate = False\n",
    "libs_to_load = sorted({\n",
    "    \"oldN2\",\n",
    "    # \"oldS6\",\n",
    "    \"newN2\",\n",
    "    # \"newS5\",\n",
    "    # \"newS6\",\n",
    "    # \"newS7\",\n",
    "    \"sPM57\",\n",
    "    \"sPM58\",\n",
    "    \"newerN2\"\n",
    "    })\n",
    "try:\n",
    "    if regenerate:\n",
    "        raise ValueError\n",
    "    \n",
    "    reads_df_raw_path = npCommon.find_newest_matching_file(f\"./output_files/*_{'-'.join(libs_to_load)}_5TERA.reads_df.transcripts.parquet\")\n",
    "    txn_df_raw_path = npCommon.find_newest_matching_file(f\"./output_files/*_{'-'.join(libs_to_load)}_5TERA.compressed_df.transcripts.parquet\")\n",
    "    print(f\"Found preprocessed files at:\\n\\t{reads_df_raw_path}\\nand:\\n\\t{txn_df_raw_path}\")\n",
    "    \n",
    "    txn_df_raw = pd.read_parquet(txn_df_raw_path)\n",
    "    reads_df_raw = pd.read_parquet(reads_df_raw_path)\n",
    "except ValueError:\n",
    "    print(f\"Could not find preprocessed files matching these libs: {'/'.join(libs_to_load)}\\nGoing to create new ones from scratch! This will take longer.\")\n",
    "    lib_dict = {}\n",
    "    for lib in libs_to_load:\n",
    "        lib_dict[lib] = load_flair_and_filter_assignments_with_genes(REV_CONVERSION_DICT[lib])\n",
    "    \n",
    "    reads_df_raw = pd.concat(list(lib_dict.values()))\n",
    "    \n",
    "    txn_df_raw = compress_df(reads_df_raw, keep_transcript_info=True).sort_index(level=['lib',\n",
    "                                                                                        'chr_id',\n",
    "                                                                                        'gene_id',\n",
    "                                                                                        'gene_name',\n",
    "                                                                                        'transcript_id'])\n",
    "    print(f\"Saving new parquets to speed up future runs.\")\n",
    "    reads_df_raw.to_parquet(f\"./output_files/{npCommon.get_dt()}_{'-'.join(libs_to_load)}_5TERA.reads_df.transcripts.parquet\")\n",
    "    txn_df_raw.to_parquet(f\"./output_files/{npCommon.get_dt()}_{'-'.join(libs_to_load)}_5TERA.compressed_df.transcripts.parquet\")\n",
    "print(f\"Lib load done @ {npCommon.get_dt(for_print=True)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "old_df_raw = reads_df_raw[reads_df_raw.lib.str.startswith('old')].copy()\n",
    "\n",
    "new_df_raw = reads_df_raw[reads_df_raw.lib.str.startswith('new')].copy()\n",
    "\n",
    "sPM_df_raw = reads_df_raw[reads_df_raw.lib.str.startswith('sPM')].copy()\n",
    "\n",
    "\n",
    "old_txn_df = compress_df(old_df_raw, keep_transcript_info=True).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name', 'transcript_id'])\n",
    "old_gene_df = compress_df(old_df_raw, keep_transcript_info=False).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name'])\n",
    "\n",
    "new_txn_df = compress_df(new_df_raw, keep_transcript_info=True).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name', 'transcript_id'])\n",
    "new_gene_df = compress_df(new_df_raw, keep_transcript_info=False).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name'])\n",
    "\n",
    "sPM_txn_df = compress_df(sPM_df_raw, keep_transcript_info=True).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name', 'transcript_id'])\n",
    "sPM_gene_df = compress_df(sPM_df_raw, keep_transcript_info=False).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reproducibility check:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "col_targets = ['gene_rpm', 'mean_polya_length', 'gene_hits', 'mean_read_length', 'gene_proteinCoding_rpm']\n",
    "new_wide_df = long_to_wide(new_gene_df, wide_target_cols=col_targets).sort_values('gene_rpm_newN2', ascending=False)\n",
    "triple_wide_df_raw = new_wide_df.merge(long_to_wide(old_gene_df, wide_target_cols=col_targets), on=['chr_id', 'gene_id', 'gene_name', 't5'])\n",
    "super_wide_df_raw = triple_wide_df_raw.merge(long_to_wide(sPM_gene_df, wide_target_cols=col_targets), on=['chr_id', 'gene_id', 'gene_name', 't5'])\n",
    "print(new_gene_df.columns)\n",
    "print(super_wide_df_raw.columns)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "super_wide_df = super_wide_df_raw.copy(deep=True)\n",
    "triple_wide_df = triple_wide_df_raw.copy(deep=True)\n",
    "\n",
    "cutoff = 5\n",
    "prefix_target = \"gene_hits\"\n",
    "for lib in [\"oldN2\", \"newN2\", \"newerN2\", \"sPM57\", \"sPM58\"]:\n",
    "    super_wide_df = super_wide_df.query(f\"{prefix_target}_{lib} >= {cutoff}\")\n",
    "    if lib.endswith(\"N2\"):\n",
    "        triple_wide_df = triple_wide_df.query(f\"{prefix_target}_{lib} >= {cutoff}\")\n",
    "super_wide_df['is_MtDNA'] = super_wide_df_raw['chr_id'] == 'MtDNA'\n",
    "\n",
    "\n",
    "sea.set_style('ticks')\n",
    "sea.set_context('talk')\n",
    "\n",
    "prefix_to_plot = \"gene_proteinCoding_rpm\"  # \"mean_read_length\" #  \"gene_rpm\" # \"mean_polya_length\"  # \"gene_proteinCoding_rpm\"\n",
    "\n",
    "cols_to_plot = [col for col in triple_wide_df.columns if col.startswith(prefix_to_plot)]\n",
    "g = sea.PairGrid(super_wide_df,\n",
    "                 vars=cols_to_plot,\n",
    "                 diag_sharey=True,\n",
    "                 corner=True,\n",
    "                 hue='is_MtDNA',\n",
    "                 palette=['0.2', 'r'],\n",
    "                 )\n",
    "if prefix_to_plot in ('gene_rpm', 'gene_proteinCoding_rpm'):\n",
    "    g.set(xscale='log',\n",
    "          yscale='log',\n",
    "          )\n",
    "# else:\n",
    "#     g.set(xscale='log',\n",
    "#       yscale='log',\n",
    "#       )\n",
    "# g.map_diag(sea.ecdfplot,\n",
    "#            # sea.kdeplot,\n",
    "#            # color='0.2',\n",
    "#            )\n",
    "g.map_lower(sea.scatterplot,\n",
    "            alpha=0.4, marker='+',\n",
    "            # color='0.2',\n",
    "            )\n",
    "# g.map_upper(sea.histplot,\n",
    "#             # color='0.0',\n",
    "#             )\n",
    "\n",
    "g.figure.supxlabel(prefix_to_plot)\n",
    "g.figure.supylabel(prefix_to_plot)\n",
    "###\n",
    "# Get axis labels for each subplot\n",
    "real_axes=[]\n",
    "for ax in g.axes.flat:\n",
    "    if ax:\n",
    "        real_axes.append(ax)\n",
    "        x_label = ax.get_xlabel()\n",
    "        if x_label:\n",
    "            ax.set_xlabel(x_label.rsplit('_')[-1])\n",
    "        y_label = ax.get_ylabel()\n",
    "        if y_label:\n",
    "            ax.set_ylabel(y_label.rsplit('_')[-1])\n",
    "real_axes[-1].get_shared_x_axes().join(*real_axes)\n",
    "real_axes[-1].get_shared_y_axes().join(*real_axes)\n",
    "xlims = real_axes[-1].get_xlim()\n",
    "ylims = real_axes[-1].get_ylim()\n",
    "min_lim = min(xlims[0], ylims[0])\n",
    "max_lim = max(xlims[1], ylims[1])\n",
    "real_axes[-1].set_xlim(min_lim, max_lim)\n",
    "real_axes[-1].set_ylim(min_lim, max_lim)\n",
    "###\n",
    "plt.tight_layout()\n",
    "\n",
    "save_dir = f\"/home/marcus/Insync/marcus.viscardi@gmail.com/Google Drive\" \\\n",
    "           f\"/insync_folder/5TERA_ReadsAndTails_Plots/raw_figures_from_python\" \\\n",
    "           f\"/{npCommon.get_dt()}_N2_reproducibility\"\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(exist_ok=True)\n",
    "save_path = save_dir / f'scatterMatrix_{prefix_to_plot}'\n",
    "for file_type in ('.svg', '.png'):\n",
    "    plt.savefig(str(save_path) + file_type,\n",
    "                # dpi=150,\n",
    "                )\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Rewrite of barplot script:\n",
    "\n",
    "This is just an effort to rewrite some old code in a less verbose way! Took some effort but works great!\n",
    "\n",
    "The down-side here is that the gene level plots are actually only showing reads that successfully had their isoforms identified. There are a good number of reads that were unable to assign isoforms because their ambiguous, but they still were obviously the target gene!!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_gene = 'rpl-3'\n",
    "context = 'talk'  # 'paper' 'talk' 'poster'\n",
    "\n",
    "plot_column_suffix = \"proteinCoding_rpm\"  # \"t5group_rpm\" or \"rpm\" or \"proteinCoding_rpm\"\n",
    "\n",
    "adapted_species_adjustment_factor = 0.1\n",
    "show_adapted_species = True\n",
    "\n",
    "nmd_sensitive_genes_and_txns = (('ubl-1', ['H06I04.4b.1']),\n",
    "                                ('rpl-3', ['F13B10.2b']),\n",
    "                                ('odc-1', ['K11C4.4.1']),\n",
    "                                ('rpl-12', ['JC8.3c.2', 'JC8.3b']),\n",
    "                                ('rpl-30', ['Y106G6H.3c.1']),\n",
    "                                ('rpl-1', ['Y71F9AL.13b.4', 'Y71F9AL.13b.2']),\n",
    "                                ('rpl-26', ['F28C6.7b.1']),\n",
    "                                ('rps-22', ['F53A3.3b.1']),\n",
    "                                )\n",
    "\n",
    "sea.set_style(\"whitegrid\")\n",
    "sea.set_context(context)\n",
    "if context == 'paper':\n",
    "    fig_size = (8, 8)\n",
    "if context == 'talk':\n",
    "    fig_size = (10, 10)\n",
    "if context == 'poster':\n",
    "    fig_size = (12, 12)\n",
    "\n",
    "\n",
    "for target_gene, _ in nmd_sensitive_genes_and_txns:\n",
    "    old_df = old_df_raw.copy(deep=True)\n",
    "    new_df = new_df_raw.copy(deep=True)\n",
    "    for potential_target_gene, NMD_sensitive_txns in nmd_sensitive_genes_and_txns:\n",
    "        if target_gene == potential_target_gene:\n",
    "            old_df['NMD Sensitive Isoform'] = old_df['transcript_id'].isin(NMD_sensitive_txns)\n",
    "            new_df['NMD Sensitive Isoform'] = new_df['transcript_id'].isin(NMD_sensitive_txns)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=fig_size,\n",
    "                             sharex=False,\n",
    "                             sharey='row')\n",
    "    \n",
    "    for row, libs_name in enumerate(['pilot', 'new']):\n",
    "        if libs_name == 'pilot':\n",
    "            df = old_df\n",
    "        else:\n",
    "            df = new_df\n",
    "        for col, gene_or_transcript_level in enumerate(['gene', 'transcript']):\n",
    "            cols_to_index = ['lib', 'chr_id', 'gene_id', 'gene_name']\n",
    "            reset_index_level = ['lib']\n",
    "            if gene_or_transcript_level == 'transcript':\n",
    "                cols_to_index.append('NMD Sensitive Isoform')\n",
    "                additional_groupby = ['NMD Sensitive Isoform']\n",
    "                reset_index_level.append('NMD Sensitive Isoform')\n",
    "            else:\n",
    "                additional_groupby = None\n",
    "            \n",
    "            plot_df = compress_df(df,\n",
    "                                  keep_transcript_info=False,  # The 'NMD Sensitive Isoform' column will retain this info here!\n",
    "                                  additional_groupby_columns=additional_groupby,\n",
    "                                  group_by_t5=True,\n",
    "                         ).sort_index(level=cols_to_index)\n",
    "            adapt_df = plot_df.query(f\"gene_name == '{target_gene}'\").xs('+', level='t5')[f'gene_{plot_column_suffix}'] / adapted_species_adjustment_factor\n",
    "            unadapt_df = plot_df.query(f\"gene_name == '{target_gene}'\").xs('-', level='t5')[f'gene_{plot_column_suffix}']\n",
    "            plot_df = pd.merge(unadapt_df,adapt_df, on=adapt_df.index.names, how='outer',\n",
    "                               suffixes=('_unadapted', '_adapted')).fillna(0)\n",
    "            if gene_or_transcript_level == 'transcript':\n",
    "                hue_col = plot_df.index.get_level_values('NMD Sensitive Isoform')\n",
    "            else:\n",
    "                hue_col = None\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            bar_names = [\"_\".join(map(str, a)) for a in zip(*[list(plot_df.index.get_level_values(level)) for level in reset_index_level])]\n",
    "            bar_unadapted_heights = plot_df[f'gene_{plot_column_suffix}_unadapted']\n",
    "            bar_adapted_heights = plot_df[f'gene_{plot_column_suffix}_adapted']\n",
    "            print(bar_names)\n",
    "            axes[row][col].bar(bar_names,\n",
    "                               bar_unadapted_heights,\n",
    "                               1,\n",
    "                               color=['orange' if 'True' in label else 'black' for label in bar_names])\n",
    "            if show_adapted_species:\n",
    "                axes[row][col].bar(bar_names,\n",
    "                                   bar_adapted_heights,\n",
    "                                   1,\n",
    "                                   bottom=bar_unadapted_heights,\n",
    "                                   color='red')\n",
    "            axes[row][col].set_title(f\"{gene_or_transcript_level.title()} RPMs on\\n{target_gene} in {libs_name} libs\")\n",
    "            axes[row][col].set_xticks(bar_names, bar_names, rotation=45, ha='right')\n",
    "            axes[row][col].set_ylabel(plot_column_suffix)\n",
    "            # print(plot_df[f'gene_{plot_column_suffix}_adapted'] / (plot_df[f'gene_{plot_column_suffix}_unadapted'] + plot_df[f'gene_{plot_column_suffix}_adapted']))\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"./output_files/isoform_plots/{npCommon.get_dt()}_{target_gene}_{plot_column_suffix}.new.barPlots\"\n",
    "    for file_type in ['.svg', '.png']:\n",
    "        plt.savefig(save_path + file_type,\n",
    "                    dpi=300)\n",
    "    plt.show()\n",
    "plot_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Tails of isoforms\n",
    "\n",
    "This shows a very interesting effect! It looks like most NMD target transcripts captured are nacent mRNAs, that have yet to reach the \"steady state\" tail lengths that we see with non-NMD target transcripts. We assume these non-target reads are coming from mRNAs that have entered the translation pool.\n",
    "\n",
    "### Feb 22, 2023 Notes:\n",
    "I have a few ToDo's remaining for this analysis:\n",
    "1. First, Josh reccomended that I try to subsample the \"steady state\" mRNA population (the non-NMD targets) and see how often I end up with a distribution similar to the adapted (post cleavage) NMD target reads. This would allow me to get an idea of the variabilty of the technique, and ifwhat I am seeing with sPM57/58 is within noise.\n",
    "2. Second, I should try and set up some kind of \"full length\" cutoff for all of the reads that are not adapted. This would let me account for the imcomplete ligation of the 5TERA adapter that I thnk I am getting!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib\n",
    "def plot_tails_from_isoforms(target_gene, df_to_plot, plot_type='ecdfPlot', show_adapted_species = False, save_dir=\".\", width_per_lib=4, height=8):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param target_gene:\n",
    "    :param plot_type: 'boxPlot' or 'ecdfPlot' or 'violinPlot'\n",
    "    :param show_adapted_species: True or False\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    lib_set = \"all\"\n",
    "    target_df = df_to_plot\n",
    "    plot_df = target_df.set_index(['lib', 'gene_id', 'gene_name', 't5', 'transcript_id']).xs((target_gene, '-'), level=(\n",
    "    'gene_name', 't5')).reset_index()\n",
    "    for potential_target_gene, NMD_sensitive_txns in nmd_sensitive_genes_and_txns:\n",
    "        if target_gene == potential_target_gene:\n",
    "            plot_df['NMD Sensitive Isoform'] = plot_df['transcript_id'].isin(NMD_sensitive_txns)\n",
    "    unique_libs = plot_df.lib.unique()\n",
    "    unique_lib_count = len(unique_libs)\n",
    "    fig, axes = plt.subplots(1, unique_lib_count, figsize=(width_per_lib * unique_lib_count, height),\n",
    "                             sharex='all',\n",
    "                             sharey='all')\n",
    "    plt.suptitle(f\"Poly(A) Tail Lengths for {target_gene}\")\n",
    "    sea.set_style(\"whitegrid\")\n",
    "    sea.set_context(\"poster\")\n",
    "    ticks = [10, 100, 1000]\n",
    "    tick_line_factors = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    tick_lines = [a * b for a, b in product(ticks, tick_line_factors)]\n",
    "    tick_labels = [str(int(label)) if label in ticks else '' for label in tick_lines]\n",
    "    for i, unique_lib in enumerate(unique_libs):\n",
    "        if plot_type == 'boxPlot':\n",
    "            sea.boxplot(plot_df.query(f\"lib == '{unique_lib}'\"),\n",
    "                        y='polya_length',\n",
    "                        x='NMD Sensitive Isoform',\n",
    "                        ax=axes[i])\n",
    "            axes[i].set_yscale('log')\n",
    "            axes[i].set_yticks(tick_lines)\n",
    "            axes[i].set_ylim((2, 500))\n",
    "            axes[i].set_yticklabels(tick_labels)\n",
    "        elif plot_type == 'violinPlot':\n",
    "            try:\n",
    "                sea.violinplot(plot_df.query(f\"lib == '{unique_lib}'\"),\n",
    "                               y='polya_length',\n",
    "                               x='lib',\n",
    "                               hue='NMD Sensitive Isoform',\n",
    "                               ax=axes[i],\n",
    "                               split=True)\n",
    "            except ValueError:\n",
    "                sea.violinplot(plot_df.query(f\"lib == '{unique_lib}'\"),\n",
    "                               y='polya_length',\n",
    "                               x='lib',\n",
    "                               hue='NMD Sensitive Isoform',\n",
    "                               ax=axes[i])\n",
    "            axes[i].set_yscale('log')\n",
    "            axes[i].set_yticks(tick_lines)\n",
    "            axes[i].set_yticklabels(tick_labels)\n",
    "            axes[i].get_legend().remove()\n",
    "        elif plot_type == 'ecdfPlot':\n",
    "            sea.ecdfplot(plot_df.query(f\"lib == '{unique_lib}'\"),\n",
    "                         x='polya_length',\n",
    "                         hue='NMD Sensitive Isoform',\n",
    "                         ax=axes[i],\n",
    "                         linewidth=5,\n",
    "                         )\n",
    "            axes[i].set_xscale('log')\n",
    "            axes[i].set_xticks(tick_lines)\n",
    "            axes[i].set_xticklabels(tick_labels)\n",
    "            axes[i].set_xlim((5, 500))\n",
    "            axes[i].get_legend().remove()\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f\"{plot_type} is not currently supported! b/c I haven't coded it or you have a typo...\")\n",
    "        lib_converter = {'oldN2': 'Wildtype (pilot)',\n",
    "                         'oldS6': 'Δsmg-6 (pilot)',\n",
    "                         'newN2': 'Wildtype (new)',\n",
    "                         'newS5': 'Δsmg-5 (new)',\n",
    "                         'newS6': 'Δsmg-6 (new)',\n",
    "                         'sPM57': 'Wildtype (Parissa)',\n",
    "                         'sPM58': '~Wildtype (Parissa)',\n",
    "                         }\n",
    "        axes[i].set_title(f\"{lib_converter[unique_lib]}\")\n",
    "    if plot_type == 'ecdfPlot': \n",
    "        plt.legend([\"True\", \"False\"],\n",
    "                   bbox_to_anchor=(0.9, 0.99),\n",
    "                   loc=\"upper right\",\n",
    "                   bbox_transform=fig.transFigure, ncol=2,\n",
    "                   title=f'NMD Sensitive Isoform',\n",
    "                   handlelength=1.5,\n",
    "                   fontsize='x-small',\n",
    "                   title_fontsize='x-small',)\n",
    "    elif plot_type == 'violinPlot':\n",
    "        handles, labels = axes[-1].get_legend_handles_labels()\n",
    "        fig.legend(handles, labels, bbox_to_anchor=(1, 1),\n",
    "                   loc=\"upper right\",\n",
    "                   bbox_transform=fig.transFigure, ncol=2,\n",
    "                   title=f'NMD Sensitive Isoform',\n",
    "                   fontsize='x-small',\n",
    "                   title_fontsize='x-small')\n",
    "    plt.tight_layout()\n",
    "    save_path = f\"{save_dir}/{npCommon.get_dt()}_{target_gene}_tails_{lib_set}.{plot_type}\"\n",
    "    for file_type in ['.svg', '.png']:\n",
    "        plt.savefig(save_path + file_type,\n",
    "                    dpi=300)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "nmd_sensitive_genes_and_txns = (# ('ubl-1', ['H06I04.4b.1']),\n",
    "                                #('rpl-3', ['F13B10.2b']),\n",
    "                                #('odc-1', ['K11C4.4.1']),\n",
    "                                # ('rpl-12', ['JC8.3c.2', 'JC8.3b']),\n",
    "                                #('rpl-30', ['Y106G6H.3c.1']),\n",
    "                                #('rpl-1', ['Y71F9AL.13b.4', 'Y71F9AL.13b.2']),\n",
    "                                ('rpl-26', ['F28C6.7b.1']),\n",
    "                                )\n",
    "\n",
    "target_genes = [gene for gene, _ in nmd_sensitive_genes_and_txns]\n",
    "plot_types = [\n",
    "    'ecdfPlot',\n",
    "    'boxPlot',\n",
    "    'violinPlot',\n",
    "]\n",
    "\n",
    "for gene, plot_kind in product(target_genes, plot_types):\n",
    "    target_gene_folder_path = f\"./output_files/isoform_plots/{npCommon.get_dt()}_tailPlots_from_{gene}\"\n",
    "    try:\n",
    "        os.mkdir(target_gene_folder_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    plot_df = reads_df_raw.copy(deep=True)\n",
    "    plot_tails_from_isoforms(gene, plot_df, plot_type=plot_kind,\n",
    "                             save_dir=target_gene_folder_path,\n",
    "                             width_per_lib=3.5,\n",
    "                             height=8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Long to wide tool\n",
    "Let's try and see if I can write a simple long_to_wide tool\n",
    "\n",
    "This would be great for my general ability to split libs up and compare across them\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = reads_df_raw.copy(deep=True)\n",
    "comp_df = compress_df(df,\n",
    "                      keep_transcript_info=False,).sort_index(level=['lib', 'chr_id', 'gene_id', 'gene_name'])\n",
    "# print(comp_df.columns)\n",
    "long_to_wide(comp_df, wide_target_cols=['gene_rpm', 'mean_polya_length'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scatter plot of isoform RPMS\n",
    "Make a scatter plot of isoforms and their RPMs between N2 and smg-5/6 treatments. Maybe this will better pick out things like rpl-12 which I have missed but is obviously a great target!\n",
    "\n",
    "This will be a little annoying b/c i'll need to split and re-merge the columns!\n",
    "\n",
    "#\n",
    "## Better option (In next cell down!):\n",
    "I think the above design for a scatter doesn't work because the actual isoforms of interest are not changing that dramatically. A better way to plot this would be to figure out the fraction that gene makes up of the total RPM, then plot that ratio between two libs. \n",
    "\n",
    "A NMD sensitive isoform would light up in this analysis because it's suddenly making up a way larger portion of the total gene's RPM when I break NMD vs in WT."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plot_col = 'transcript_rpm'\n",
    "\n",
    "df = reads_df_raw.query(\"lib == 'oldN2' | lib == 'oldS6'\").copy(deep=True)\n",
    "compressed_df = compress_df(df, group_by_t5=False)\n",
    "\n",
    "lib_names = compressed_df.index.get_level_values('lib').to_series().unique()\n",
    "\n",
    "dfs = []\n",
    "for lib in lib_names:\n",
    "    dfs.append(compressed_df.query(f\"lib == '{lib}'\").reset_index(level='lib')[plot_col].rename(f\"{plot_col}_{lib}\"))\n",
    "plot_df = pd.concat(dfs, axis=1).fillna(0)\n",
    "\n",
    "fig = px.scatter(plot_df.reset_index(),\n",
    "                 x=f'{plot_col}_{lib_names[0]}',\n",
    "                 y=f'{plot_col}_{lib_names[1]}',\n",
    "                 hover_name='gene_name',\n",
    "                 hover_data=['transcript_id'])\n",
    "fig.show(renderer='browser')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trying to plot change in isoform usage within genes\n",
    "\n",
    "Like noted in the above markdown cell. This is kinda a weird analysis but it does capture different information than the rocket plots!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = reads_df_raw.query(\"lib == 'oldN2' | lib == 'oldS6'\").copy(deep=True)\n",
    "compressed_df = compress_df(df, group_by_t5=False)\n",
    "\n",
    "mini_df = compressed_df.sort_values([\"lib\", \"gene_id\"])[['transcript_hits', 'transcript_rpm', 'transcript_proteinCoding_rpm']]\n",
    "# Cool way to do this from: https://stackoverflow.com/questions/23377108/pandas-percentage-of-total-with-groupby\n",
    "mini_df['fraction_of_gene_rpm'] = mini_df['transcript_rpm'] / mini_df.groupby(['lib', 'gene_id'])['transcript_rpm'].transform('sum')\n",
    "mini_df['test_col'] = mini_df['fraction_of_gene_rpm'] * mini_df.groupby(['lib', 'gene_id'])['transcript_rpm'].transform('sum')\n",
    "\n",
    "wide_mini_df = long_to_wide(mini_df, wide_target_cols=['fraction_of_gene_rpm', 'transcript_rpm'])\n",
    "wide_mini_df['combined_rpm'] = wide_mini_df.transcript_rpm_oldN2 + wide_mini_df.transcript_rpm_oldS6 + 5\n",
    "wide_mini_df = wide_mini_df.query(\"combined_rpm >= 100\").query(\"fraction_of_gene_rpm_oldN2 + fraction_of_gene_rpm_oldS6 != 2\")\n",
    "wide_mini_df['long_name'] = wide_mini_df.gene_name + \" (\" + wide_mini_df.transcript_id + \")\"\n",
    "\n",
    "fig = px.scatter(wide_mini_df,\n",
    "                 x='fraction_of_gene_rpm_oldN2',\n",
    "                 y='fraction_of_gene_rpm_oldS6',\n",
    "                 size='combined_rpm',\n",
    "                 size_max=25,\n",
    "                 hover_name='long_name')\n",
    "fig.show(renderer='browser')"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Trying to combine tail and isoform information\n",
    "So for some genes like rpl-30, the splice junction is long and ahead of the NMD eliciting STOP so that most of the degradation intermediates retain splice information. This makes it clear that they came from the NMD isoform! Genes like ubl-1 are very much the opposite case, in that all degradation intermediates are completely ambiguous as it if they came from NMD or not... *(but we ~know they did)*\n",
    "\n",
    "I want to try and get code working that is able to take advantage of these few genes where deg. intermediates retain isoform information. That's what this next section will do.\n",
    "\n",
    "Two large pieces fall into this:\n",
    "1. We have to make sure that reads that do not have enough information to confidently ID the right isoform are not arbitrarily getting assigned\n",
    "2. Plot tail lengths with the combinations of adapted/unadapted and NMD sensitive/insensitive\n",
    "\n",
    "\n",
    "### Feb 23, 2023:\n",
    "I should really try and get a bootstrapping test of this going to see how often I see the NMD adapted type tails coming out of the unadapted NMD insensitive reads\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "log_xaxis = True\n",
    "\n",
    "target_libs = (# \"newN2\",\n",
    "               \"oldN2\",\n",
    "               \"oldS6\",\n",
    "               # \"newS6\",\n",
    "               # \"newS5\",\n",
    "               # \"sPM57\",\n",
    "               # \"sPM58\",\n",
    "               )\n",
    "\n",
    "# At the isoform edge position we can differentiate cleavage species' parent isoform, left of which, we have enough info to differentiate between the NMD sensitive or insenstive isoforms!\n",
    "# I manually identified that same isoform edge information for the following genes:\n",
    "targets_and_iso_edge_dict = {\"rpl-30\": [10_436_409, 'left'],\n",
    "                             # \"rpl-26\": [8_603_272, 'left'],\n",
    "                             # \"rpl-3\": [3_868_327, 'left'],\n",
    "                             # \"rpl-1\": [2_876_019, 'right'],\n",
    "                             # \"rpl-12\": [13_240_023, 'right'],\n",
    "                             \"ubl-1\": [3_068_573, 'left'],\n",
    "                             \"rps-22\": [1_950_996, 'left']\n",
    "                             }\n",
    "\n",
    "nmd_sensitive_genes_and_txns = (('ubl-1', ['H06I04.4b.1']),\n",
    "                                ('rpl-3', ['F13B10.2b']),\n",
    "                                ('odc-1', ['K11C4.4.1']),  # This analysis isn't quite the same...\n",
    "                                ('rpl-12', ['JC8.3c.2', 'JC8.3b']),\n",
    "                                ('rpl-30', ['Y106G6H.3c.1']),\n",
    "                                ('rpl-1', ['Y71F9AL.13b.4', 'Y71F9AL.13b.2']),\n",
    "                                ('rpl-26', ['F28C6.7b.1']),\n",
    "                                ('rps-22', ['F53A3.3b.1']),\n",
    "                                )\n",
    "\n",
    "sea.set_style(\"whitegrid\")\n",
    "sea.set_context(\"poster\")\n",
    "\n",
    "palette = {\"NMD'ed: +  t5: -\":\"darkred\",\n",
    "           \"NMD'ed: +  t5: +\":\"red\", \n",
    "           \"NMD'ed: -  t5: -\":\"darkblue\",\n",
    "           \"NMD'ed: -  t5: +\":\"blue\"}\n",
    "\n",
    "\n",
    "def plots_tails(library_df, targeted_gene, lib_target, save_dir=f\"./output_files/isoform_plots/NMD_and_Adapted_tailPlots\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    g = sea.ecdfplot(library_df.sort_values('NMD_and_t5'),\n",
    "                     ax=ax,\n",
    "                     x='polya_length',\n",
    "                     hue='NMD_and_t5',\n",
    "                     palette=palette,\n",
    "                     log_scale=log_xaxis,\n",
    "                     )\n",
    "    # Change the line styles:\n",
    "    for i, line in enumerate(g.lines):\n",
    "        if line.get_color().startswith(\"dark\"):\n",
    "            line.set_linestyle(\"-\")\n",
    "        else:\n",
    "            line.set_linestyle(\"--\")\n",
    "    plt.title(f\"Tail lengths from {lib_target} for {targeted_gene}\")\n",
    "    if log_xaxis:\n",
    "        ax.set_xbound(10, 200)\n",
    "        ax.set_xticks([10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200])\n",
    "        ax.set_xticklabels([10, 20, 30, 40, 50, None, 70, None, None, 100, 200])\n",
    "    else:\n",
    "        ax.set_xbound(-1, 200)\n",
    "    # sea.move_legend(g, \"lower right\")\n",
    "    ax.get_legend().remove()\n",
    "    plt.tight_layout()\n",
    "    save_dir = Path(save_dir)\n",
    "    if not save_dir.exists():\n",
    "        print(f\"Making new directory at: {save_dir}\")\n",
    "        save_dir.mkdir()\n",
    "    \n",
    "    save_path = str(save_dir) + f\"/{npCommon.get_dt()}_{targeted_gene}_{lib_target}_NMD-and-t5_tailLength.ecdf\"\n",
    "    if log_xaxis:\n",
    "        save_path += \".logAxis\"\n",
    "    for file_type in ['.svg', '.png']:\n",
    "        plt.savefig(save_path + file_type, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for target_gene, (iso_edge, left_or_right) in targets_and_iso_edge_dict.items():\n",
    "    if left_or_right == 'left':\n",
    "        comparator = \"<=\"\n",
    "        anti_comparator = \">=\"\n",
    "    elif left_or_right == 'right':\n",
    "        comparator = \">=\"\n",
    "        anti_comparator = \"<=\"\n",
    "    else:  # This is just to shut pycharm up\n",
    "        comparator = None\n",
    "        anti_comparator = None\n",
    "    if target_gene == 'ubl-1':\n",
    "        df = reads_df_raw.copy(deep=True).query(f\"gene_name == '{target_gene}'\")\n",
    "    else:\n",
    "        df = reads_df_raw.copy(deep=True).query(f\"gene_name == '{target_gene}'\").query(f\"chr_pos {comparator} {iso_edge}\")\n",
    "    nmd_sensitive_genes_and_txns_dict = {potential_target_gene: NMD_sensitive_txns for potential_target_gene, NMD_sensitive_txns in nmd_sensitive_genes_and_txns}\n",
    "    df['NMD_Sensitive_Isoform'] = df['transcript_id'].isin(nmd_sensitive_genes_and_txns_dict[target_gene]).replace({True: '+', False: '-'})\n",
    "    if target_gene == 'ubl-1':\n",
    "        df.NMD_Sensitive_Isoform.mask(((df.t5 == '+') & (df.eval(f\"chr_pos {anti_comparator} {iso_edge}\"))), '+', inplace=True)\n",
    "    \n",
    "    for lib_for_tail_plot in df.lib.unique():\n",
    "        if lib_for_tail_plot not in target_libs:\n",
    "            continue\n",
    "        print(lib_for_tail_plot, target_gene)\n",
    "        lib_df = df.query(f\"lib == '{lib_for_tail_plot}'\").copy()\n",
    "        lib_df['NMD_and_t5'] = lib_df.apply(lambda row: f\"NMD'ed: {str(row['NMD_Sensitive_Isoform'])}  t5: {row['t5']}\", axis=1)\n",
    "\n",
    "        plots_tails(lib_df, target_gene, lib_for_tail_plot,\n",
    "                    save_dir=f\"/home/marcus/Insync/marcus.viscardi@gmail.com/Google Drive/insync_folder/5TERA_ReadsAndTails_Plots/raw_figures_from_python/{npCommon.get_dt()}_isoformTailPlots_{'-'.join(target_libs)}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read Plotting W/ Isoforms\n",
    "\n",
    "The goal here is to run my read plotting and coverage plotting scripts, but leverage the additional isoform information from FLAIR."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make it prettier\n",
    "\n",
    "The below code shows how to make subsets of plots, which might be really nice for making the plots more manageable!\n",
    "[Link to StackOverflow where I got this from!](https://stackoverflow.com/a/67694491/13316742)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from nanoporeReadPlotting.finalizingReadAndCoveragePlotting_matplotlib import coverage_plotting_5tera\n",
    "\n",
    "sea.set_style(\"whitegrid\")\n",
    "sea.set_context(\"talk\")\n",
    "\n",
    "target_genes = {# \"rpl-30\",\n",
    "                # \"rpl-26\",\n",
    "                # \"rpl-3\",\n",
    "                # \"rpl-1\",\n",
    "                # \"rpl-12\",\n",
    "                \"ubl-1\",\n",
    "                # 'aly-3',\n",
    "                # 'K08D12.3',\n",
    "                # 'hel-1',\n",
    "                # 'rsp-1',\n",
    "                # 'rps-22',\n",
    "                }\n",
    "\n",
    "iso_edge_dict = {\n",
    "    # gene: [edge, compare, anticompare]\n",
    "    \"rpl-30\": [10_436_409, \"<=\", \">=\"],\n",
    "    \"rpl-26\": [8_603_272, \"<=\", \">=\"],\n",
    "    \"rpl-3\": [3_868_327, \"<=\", \">=\"],\n",
    "    \"rpl-1\": [2_876_019, \">=\", \"<=\"],\n",
    "    \"rpl-12\": [13_240_023, \">=\", \"<=\"],\n",
    "    \"ubl-1\": [3_068_573, \"<=\", \">=\"],\n",
    "    \"rps-22\": [1_950_996, \"<=\", \">=\"]\n",
    "    }\n",
    "\n",
    "nmd_sensitive_genes_and_txns = (('ubl-1', ['H06I04.4b.1']),\n",
    "                                ('rpl-3', ['F13B10.2b']),\n",
    "                                ('odc-1', ['K11C4.4.1']),  # This analysis isn't quite the same... b/c only 1 isoform\n",
    "                                ('rpl-12', ['JC8.3c.2', 'JC8.3b']),\n",
    "                                ('rpl-30', ['Y106G6H.3c.1']),\n",
    "                                ('rpl-1', ['Y71F9AL.13b.4', 'Y71F9AL.13b.2']),\n",
    "                                ('rpl-26', ['F28C6.7b.1']),\n",
    "                                ('rps-22', ['F53A3.3b.1']),\n",
    "                                ('rsp-1', ['W02B12.3b.2']),\n",
    "                                ('hel-1', ['C26D10.2b']),\n",
    "                                ('K08D12.3', ['K08D12.3b.1']),\n",
    "                                ('aly-3', ['M18.7b']),\n",
    "                                )\n",
    "\n",
    "compare_libs_sets = ((\"oldN2\",),\n",
    "                     (\"oldN2\", \"oldS6\"),\n",
    "                     # (\"sPM57\", \"sPM58\"),\n",
    "                     # (\"newN2\", \"oldN2\", \"newS5\", \"newS6\", \"oldS6\"),\n",
    "                     )\n",
    "\n",
    "def plot_coverages_for_isoforms(plot_df, target_gene, compare_libs,\n",
    "                                targets_and_nontargets_side_by_side=True,\n",
    "                                plot_ambiguous_reads=False,\n",
    "                                quiet=True, save_dir=None):\n",
    "    num_compare_libs = len(compare_libs)\n",
    "    if not quiet:\n",
    "        print(f\"comparing {num_compare_libs} libraries: {' '.join(compare_libs)}\")\n",
    "    if targets_and_nontargets_side_by_side:\n",
    "        fig_size = [5, 2.5 * num_compare_libs]  # Width, Height\n",
    "        if plot_ambiguous_reads:\n",
    "            fig_size[1] += 1.25 * num_compare_libs\n",
    "        outer_row_num, outer_col_num = num_compare_libs, 1\n",
    "    else:\n",
    "        fig_size = [3.5 * num_compare_libs, 5]  # Width, Height\n",
    "        if plot_ambiguous_reads:\n",
    "            fig_size[1] += 2.5\n",
    "        outer_row_num, outer_col_num = 1, num_compare_libs\n",
    "    \n",
    "    fig = plt.figure(constrained_layout=True,\n",
    "                     figsize=fig_size,\n",
    "                     )\n",
    "    # This will produce sets of sub-figures that we can build up figures inside! These could be 1 'subfig' per lib.\n",
    "    subfigs = fig.subfigures(outer_row_num,\n",
    "                             outer_col_num)  # Rows, Cols. This could work the other way too!\n",
    "    \n",
    "    if num_compare_libs == 1:\n",
    "        flat_subfigs = [subfigs]\n",
    "    else:\n",
    "        flat_subfigs = subfigs.flat\n",
    "    # This will allow me to save all of these into a list, then I can force them to all share Y-scale!\n",
    "    adapted_axes = []\n",
    "    unadapted_axes = []\n",
    "    for outer_index, (subfig, lib) in enumerate(zip(flat_subfigs, compare_libs)):\n",
    "        subfig.suptitle(lib)\n",
    "        lib_df = plot_df.query(f\"lib == '{lib}'\").copy()\n",
    "\n",
    "        if targets_and_nontargets_side_by_side:\n",
    "            subfig_rows, subfig_cols = 2, 2\n",
    "            height_ratios = [1, 2]\n",
    "            if plot_ambiguous_reads:\n",
    "                subfig_cols += 1\n",
    "        else:\n",
    "            subfig_rows, subfig_cols = 4, 1\n",
    "            height_ratios = [1, 2, 1, 2]\n",
    "            if plot_ambiguous_reads:\n",
    "                subfig_rows += 2\n",
    "                height_ratios += [1, 2]\n",
    "\n",
    "        # Other option here would be to make the libs stack: 4 rows, 1 col. The unstacking code below will handle either option!\n",
    "        axes = subfig.subplots(subfig_rows,  # num rows: adapted & unadapted\n",
    "                               subfig_cols,  # num cols: NMD sensitive and not sensitive\n",
    "                               height_ratios=height_ratios,\n",
    "                               )\n",
    "        if len(axes.shape) == 2 and axes.shape[0] == 2:\n",
    "            target_axes, nontarget_axes = axes.transpose()\n",
    "            ambiguous_axes = None\n",
    "        elif len(axes.shape) == 2 and axes.shape[0] == 3:  # In the case of plotting ambiguous reads!\n",
    "            target_axes, nontarget_axes, ambiguous_axes = axes.transpose()\n",
    "        elif len(axes.shape) == 1 and axes.shape[0] == 4:\n",
    "            target_axes, nontarget_axes = np.array_split(axes, 2)\n",
    "            ambiguous_axes = None\n",
    "        elif len(axes.shape) == 1 and axes.shape[0] == 6:  # In the case of plotting ambiguous reads!\n",
    "            target_axes, nontarget_axes, ambiguous_axes = np.array_split(axes, 3)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        NMD_target_iterator = ['+', '-']\n",
    "        axes_group_iterator = [target_axes, nontarget_axes]\n",
    "        \n",
    "        adapted_axes += [target_axes[0], nontarget_axes[0]]\n",
    "        unadapted_axes += [target_axes[1], nontarget_axes[1]]\n",
    "        if plot_ambiguous_reads:\n",
    "            adapted_axes += [ambiguous_axes[0]]\n",
    "            unadapted_axes += [ambiguous_axes[1]]\n",
    "        \n",
    "        if not plot_ambiguous_reads:\n",
    "            axes_iterator = zip(NMD_target_iterator, axes_group_iterator)\n",
    "        else:\n",
    "            NMD_target_iterator += ['~']\n",
    "            axes_group_iterator += [ambiguous_axes]\n",
    "            axes_iterator = zip(NMD_target_iterator, axes_group_iterator)\n",
    "        \n",
    "        for target, target_or_non_axes in axes_iterator:  # Either two steps here or three (if ambiguous reads getting plotted!)\n",
    "            print(f\"Calculating coverage for NMD-({target}) isoform in {lib} lib:\")\n",
    "            # print(lib_df.query(f\"NMD_Sensitive_Isoform == '{target}'\"))\n",
    "            coverage_plotting_5tera(lib_df,\n",
    "                                    gene_name=target_gene,\n",
    "                                    provide_axes=target_or_non_axes,\n",
    "                                    rpm_normalize=True,\n",
    "                                    additional_plot_df_query=f\"NMD_Sensitive_Isoform == '{target}'\",\n",
    "                                    # it's import to filter inside of this method rather than ahead of time, this is b/c the coverage_plotting script uses all the reads to calculate rpm!!\n",
    "                                    quiet=quiet,\n",
    "                                    )\n",
    "            target_or_non_axes[1].set_xlabel(f\"NMD ({target})\")\n",
    "            if targets_and_nontargets_side_by_side and target == '-':\n",
    "                target_or_non_axes[0].set_yticklabels([])\n",
    "                target_or_non_axes[1].set_yticklabels([])\n",
    "    adapted_axes[0].get_shared_y_axes().join(*adapted_axes)\n",
    "    unadapted_axes[0].get_shared_y_axes().join(*unadapted_axes)\n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    libs_string = '/'.join(compare_libs)\n",
    "    # plt.suptitle(f\"Read Coverage for {target_gene}\\nin {libs_string}\")\n",
    "    if isinstance(save_dir, str):\n",
    "        save_dir = Path(save_dir)\n",
    "        if not save_dir.exists():\n",
    "            print(f\"Making new directory at: {save_dir}\")\n",
    "            save_dir.mkdir()\n",
    "        save_path = str(save_dir) + f\"/{target_gene}_{'-'.join(compare_libs)}_coveragePlots\"\n",
    "        if plot_ambiguous_reads:\n",
    "            save_path += \"_withAmbiguouslyAssignedReads\"\n",
    "        print(f\"Saving file to {save_path}.svg/png\")\n",
    "        for file_type in ('.svg', '.png'):\n",
    "            plt.savefig(save_path + file_type)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for target_gene in target_genes:\n",
    "    print(f\"Making plots for {target_gene}\")\n",
    "    df = reads_df_raw.copy(deep=True)  #.sample(100000)  # TODO: Drop this sample step, it just made things faster\n",
    "    nmd_sensitive_genes_and_txns_dict = {potential_target_gene: NMD_sensitive_txns for potential_target_gene, NMD_sensitive_txns in nmd_sensitive_genes_and_txns}\n",
    "    df['NMD_Sensitive_Isoform'] = df['transcript_id'].isin(nmd_sensitive_genes_and_txns_dict[target_gene]).replace({True: '+', False: '-'})\n",
    "    if target_gene == 'ubl-1':\n",
    "        iso_edge, comparator, anti_comparator = iso_edge_dict[target_gene]\n",
    "        df.NMD_Sensitive_Isoform.mask(((df.t5 == '+') & (df.eval(f\"chr_pos {anti_comparator} {iso_edge}\"))), '~', inplace=True)\n",
    "    for compare_libs in compare_libs_sets:\n",
    "        plot_coverages_for_isoforms(df, target_gene, compare_libs,\n",
    "                                    targets_and_nontargets_side_by_side=False,\n",
    "                                    save_dir=f\"/home/marcus/Insync/marcus.viscardi@gmail.com/Google Drive/insync_folder/5TERA_ReadsAndTails_Plots/raw_figures_from_python/{npCommon.get_dt()}_coveragePlots3_{'-'.join(compare_libs)}\",\n",
    "                                    plot_ambiguous_reads=True,\n",
    "                                    quiet=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "target_gene = 'ubl-1'\n",
    "df = reads_df_raw.copy(deep=True)  #.sample(100000)  # TODO: Drop this sample step, it just made things faster\n",
    "nmd_sensitive_genes_and_txns_dict = {potential_target_gene: NMD_sensitive_txns for potential_target_gene, NMD_sensitive_txns in nmd_sensitive_genes_and_txns}\n",
    "df['NMD_Sensitive_Isoform'] = df['transcript_id'].isin(nmd_sensitive_genes_and_txns_dict[target_gene]).replace({True: '+', False: '-'})\n",
    "iso_edge, comparator, anti_comparator = iso_edge_dict[target_gene]\n",
    "df.NMD_Sensitive_Isoform.mask(((df.t5 == '+') & (df.eval(f\"chr_pos {anti_comparator} {iso_edge}\"))), '~', inplace=True)\n",
    "\n",
    "df.query(f\"gene_name == '{target_gene}'\").query(f\"NMD_Sensitive_Isoform == '~'\").query(f\"lib == 'oldN2'\")[['read_id', 't5', 'gene_name_original', 'transcript_id', 'gene_name', 'gene_name_fromGeneAssign', 'NMD_Sensitive_Isoform']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# *ets-4* Weirdness\n",
    "I've had some weirdness going on with *ets-4* where it doesn't seem like it's being assigned correctly. I'm going to try to explore this.\n",
    "\n",
    "I think part of what's happening is that the annotation for *ets-4* overlaps with another gene (*ceh-60*), and for whatever reason this is tanking assignments(?).\n",
    "\n",
    "For example, I can see on IGV that there are 30 clear reads in my oldN2 lib on *ets-4*. But **ZERO(?!)** of these make it to this stage... why?\n",
    "\n",
    "The only read I see from oldN2 is \"f1250dbe-d5f9-47a6-a60f-d55ff5a6f909\", and this read is obviously from *ceh-60* when you look at it on IGV. It's in the opposite direction ffs!\n",
    "\n",
    "Is FLAIR not strand aware?! That seems like a massive oversight...\n",
    "\n",
    "**I need to go look if I am using FLAIR incorrectly, or if I forgot a stand-aware flag of some sort**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "reads_df_raw.query(\"gene_name == 'ets-4' | gene_name_fromGeneAssign == 'ets-4'\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
